<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article SYSTEM "journalpub-oasis3.dtd">

<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">lens-dev</journal-id>

      <journal-title-group>
        <journal-title>Lens</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2050-084X</issn>
      <publisher>
        <publisher-name>eLife Lens</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>

      <title-group>
        <article-title>eLife Lens: A novel way of seeing content</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Grubisic</surname>
            <given-names>Ivan</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="fn" rid="con1"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Aufreiter</surname>
            <given-names>Michael</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="fn" rid="con2"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Buchtala</surname>
            <given-names>Oliver</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="fn" rid="con3"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Nott</surname>
            <given-names>Graham</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="fn" rid="con4"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Close</surname>
            <given-names>Rebecca</given-names>
          </name>
          <xref ref-type="aff" rid="aff6">6</xref>
          <xref ref-type="fn" rid="con8"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Korosec</surname>
            <given-names>Samo</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">4</xref>
          <xref ref-type="fn" rid="con6"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>Ian</given-names>
          </name>
          <xref ref-type="aff" rid="aff5">5</xref>
          <xref ref-type="fn" rid="con7"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Mulvany</surname>
            <given-names>Ian</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="fn" rid="con5"/>
        </contrib>

        <aff id="aff1">
          <label>1</label>
          <institution>Li Ka Shing Center For Biomedical and Health Sciences, CIRM Center of Excellence, University of California, Berkeley</institution>,
          <addr-line>
          <named-content content-type="city">Berkeley</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <aff id="aff2">
          <label>2</label>
          <institution>Substance</institution>,
          <addr-line>
          <named-content content-type="city">Linz</named-content>
          </addr-line>,
          <country>Austria</country>
        </aff>

        <aff id="aff3">
          <label>3</label>
          <institution>eLife Publications</institution>,
          <addr-line>
          <named-content content-type="city">Cambridge</named-content>
          </addr-line>,
          <country>United Kingdom</country>
        </aff>

        <aff id="aff4">
          <label>4</label>
          <institution>froodee design bureau</institution>,
          <addr-line>
          <named-content content-type="city">Vienna</named-content>
          </addr-line>,
          <country>Austria</country>
        </aff>

        <aff id="aff5">
          <label>5</label>
          <institution>ripe</institution>,
          <addr-line>
          <named-content content-type="city">Washington DC</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <aff id="aff6">
          <label>6</label>
          <institution>Landes Bioscience</institution>,
          <addr-line>
          <named-content content-type="city">Austin, Texas</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <fn fn-type="con" id="con1">
          <p>Conception of the idea, XML to JSON conversion, drafting and revising the article</p>
        </fn>

        <fn fn-type="con" id="con2">
          <p>Design and implementation of Lens, revising the article</p>
        </fn>

        <fn fn-type="con" id="con3">
          <p>Reference implementation of the new browser-based NLM converter</p>
        </fn>

        <fn fn-type="con" id="con4">
          <p>Implementation of the deployment workflow, revising the article</p>
        </fn>

        <fn fn-type="con" id="con5">
          <p>Project coordination</p>
        </fn>

        <fn fn-type="con" id="con6">
          <p>Advice on UX Design, mockups to support UI development</p>
        </fn>

        <fn fn-type="con" id="con7">
          <p>Advice on the UI Design, revising the article</p>
        </fn>

        <fn fn-type="con" id="con8">
          <p>Integration of Lens at Landes Bioscience. Advice on the design of the NLM converter</p>
        </fn>

      </contrib-group>
      <abstract>
        <p>eLife Lens provides a novel way of looking at content on the web (see <ext-link xlink:href="http://www.elifesciences.org/lens/">introduction post</ext-link>). It is designed to make life easier for researchers, reviewers, authors and readers. For example, have you tried to look at a figure in an online article, while simultaneously trying to see what the author says about the figure? You end up jumping all around the article, losing track of what you were looking for in the first place. The reason for this is that most online research articles are published in a fixed digital version of the original paper. With eLife Lens, we take full advantage of the dynamic nature of HTML combined with javascript (<xref ref-type="fig" rid="video1">Video 1</xref>).</p>
      </abstract>

    </article-meta>
  </front>
  <body>
    <sec id="s1" sec-type="motivation">
      <title>Motivation</title>
      <p>Working with digital documents has been difficult, primarily because they come in presentation-centric formats with the goal of appearing identical across multiple devices for printing purposes. Content today, however, is no longer being printed out readily. Instead, it is being read on a variety of platforms, from computers to tablets and other mobile devices. Although optimized for print, when presented with different screen sizes and other device inconsistencies traditional digital publishing formats lack key features that could otherwise improve readers' ability to focus on an author's arguments.</p>
      <p>The web browser provides a unified platform for viewing content. Instead of binding the content to a presentation-focused format, we can view the content as data and utilize a defined data structure to make the content readily accessible. This data structure can then be processed as if it were its own database -- allowing users to query the content, create new content and build new tools.</p>
      <p>We have decided to focus our initial efforts on applying the data-centric representation of content to scientific literature. The Open Access scientific community has standardized much of their content into a formally annotated XML format, making it easier to gain access to a large library of articles. Scientific articles are an amalgamation of text, figures, tables, videos and references which are used to form the authorâ€™s argument. Scientific arguments are also rarely linear in nature, making it difficult to follow when all of the content is not readily viewable on a digital device. The incredible amount of available data and non-linear format of the articles made them the ideal candidate for testing the flexibility of our data-centric approach. With the release of Lens, we would like to promote, not only the data-centric representation of scientific content, but of any content which would be optimized for viewing on web clients.</p>
    </sec>

    <sec id="s2" sec-type="presentation">
      <title>The Presentation of Lens</title>
      <p>When designing Lens, we worked to provide a flexible experience that supports a variety of use cases. On the far left, the document map (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) identifies the reader's position in the context of the entire article. It also outlines each paragraph in the article. The left panel includes all of the textual content of the article (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The right panel includes the resources (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The resources include the table of contents, figures, tables, videos, supplemental data, references and information about the article. By separating the content into individual panels, we allow the reader to focus on multiple content bits at the same time.</p>
      <p>When reading scientific articles, it is rare for a reader to read straight through the text and figures in order. Instead, they may look to get a quick overview of the paper by looking at the figures first. By separating the figures from the text, the reader can transition between the two independently of one another. This allows the reader to simultaneously view exactly the content they want to focus on at that moment.</p>
<!--      <p>Two viewing modes that are implicitly supported are text-centric and resource-centric viewing. Text-centric viewing creates a microscale reading experience by limiting the viewable content to a singular content node. When a figure or publication label is selected within a text node, only the figures or publications that are referenced within that paragraph will be displayed in their appropriate resource sections (<xref rid="fig3" ref-type="fig">Figure 3</xref>, <xref rid="fig4" ref-type="fig">Figure 4</xref>). Selection of any figure or publication will color the paragraphs in the document map (<xref rid="fig2" ref-type="fig">Figure 2A</xref>) that include a reference to the selected resource.</p>
-->      <p>The presentation of the article information has also been redesigned so as to organize the information in a uniform way. Each publisher will have placed information such as an authorâ€™s impact statement, article keywords, major dataset links, etc. in different positions of their HTML or PDF versions of an article. With Lens, the aim is to distill all of the article information into organized subsections that can be displayed together on individual cards. These information cards create a uniform viewing experience, making it easy to quickly find what the reader is looking for.</p>
      <p>In addition to simplifying the reading experience of research articles, Lens also provides useful tools to share content with others. Each of the content nodes in the data structure are deep-linked.  This means that each state is defined by a given URL. Sharing a specific URL will take the new reader to the exact same state in their browser which will facilitate discussions about the content.</p>
    </sec>

    <sec id="s3" sec-type="format">
    	<title>The Lens Article</title>
      <p>We are convinced that there is a need to break with traditional presentation-centric formats, by considering content as data and making it accessible in new ways. Therefore with the release of Lens, weâ€™d like to promote a data-centric representation of scientific content, optimized for consumption by web-clients, the <ext-link xlink:href="http://lens.substance.io/#lens/lens_article">Lens Article</ext-link></p>
      <p>Lens can display any document that conforms to a simple JSON representation. JSON representations are flexible and can be adapted to any type of content. The initial JSON object provides the framework for organizing all of the content in an article. The <italic>id</italic> and <italic>properties</italic> keys provide global access if many JSON articles are to be queried for an initial round of filtering. The <italic>nodes</italic> key contains all of the content and the <italic>view</italic> key defines indices describing the order in which the nodes will be displayed. The nodes are then linked together via annotations. Annotations represent either font styling or an explicit reference to a target content node that is contained in its source content code. <xref ref-type="fig" rid="fig1">Figure 1</xref> outlines a simple example of how to use the JSON representation to compose an article for Lens.</p>

      <p>The creation of an article's JSON depends on a browser-based converter that runs through the XML tags, turning them into a smart data structure to power our viewer. The XML tags define various types of content nodes. Each node contains <italic>type</italic>, <italic>id</italic> and <italic>content</italic> keys that the viewer uses to define the rendering. All of the keys have their own definitions with regard to how Lens renders them. The translation from XML to a structured JSON creates a consistent presentation scheme for the content.</p>
    </sec>

    <sec id="s4" sec-type="open_source">
    <title>Open Development</title>
    <p>Lens is open source software. Its codebase has a modular design and is available on <ext-link xlink:href="https://github.com/elifesciences/lens">Github</ext-link>. Lens relies heavily on <ext-link xlink:href="http://substance.io">Substance</ext-link>, an open platform for browser-based document manipulation. We'd like to invite anyone to use Lens for displaying their own content and to get involved in the development process. You can report bugs and discuss features on Github or post to our <ext-link xlink:href="https://groups.google.com/forum/#!forum/elife-lens">mailing list</ext-link>. If you are a developer, take a glance at the <ext-link xlink:href="http://lens.substance.io/#lens/manual">Lens Manual</ext-link>.</p>
    </sec>
	<sec id="s5" sec-type="growing_corpus">
    <title>The Future of Lens</title>
    <p>The integration to Substance has now positioned Lens to be part of a larger publishing platform. We have enabled drag and drop functionality to Lens so that any NLM XML file can be automatically converted into the Lens format. The only caveat at the moment is that the images might not render if that publisher's definitions have not been added to Lens. We currently support <italic>eLife</italic>, <italic>Landes Biosciences</italic>, <italic>PLOS</italic>, and <italic>PeerJ</italic>. If you have a favorite open access journal that you would like to see supported, please let us know via the <ext-link xlink:href="https://github.com/elifesciences/lens/issues">Issues</ext-link>.</p>
    <p>By encouraging authors and publishers to use and evolve open software components and a lightweight exchange format usable by web-clients, we reduce the obstacles that originate from economic competition and pave the way to true open publishing and an open exchange of information.</p> 
  	<p>With Substance, we would like to contribute an extensive content creation and annotation framework built on web infrastructure. It is designed to be customized and integrated into existing workflows.</p>
    <p>We think that research in general could also benefit from such a set of tools. We are also collaborating with the team at <ext-link xlink:href="http://fiduswriter.org/">Fidus Writer</ext-link>, since our projects share many ideas. Furthermore we would like to thank <ext-link xlink:href="http://hypothes.is/">Hypothes.is</ext-link> for advice and are looking forward to exploring further collaboration.</p>
    </sec>
    <sec id="s6" sec-type="credits">
    <title>Credits</title>
    <p>eLife Lens was developed in collaboration between <ext-link xlink:href="http://bioegrad.berkeley.edu/">UC Berkeley</ext-link> graduate student <ext-link xlink:href="http://www.linkedin.com/pub/ivan-grubisic/26/353/739">Ivan Grubisic</ext-link> and eLife. He was supported by Michael Aufreiter from Substance, who helped with the design and implementation of the tool. Samo KoroÅ¡ec (<ext-link xlink:href="http://dribbble.com/froodee">froodee</ext-link>) and Ian Hamilton (ripe) gave advice on the UX/UI design. <ext-link xlink:href="http://www.grahamnott.com/">Graham Nott</ext-link> implemented the deployment workflow and eLife provided support for the project and the initial corpus of documents against which the tool was developed. <ext-link xlink:href="http://www.linkedin.com/pub/rebecca-close/4a/585/224">Rebecca Close</ext-link>, integrated Lens at <ext-link xlink:href="https://www.landesbioscience.com">Landes Bioscience</ext-link> and helped with the redesign of the converter, that now supports all NLM-compatible content.</p>
    </sec>
  </body>
  <back>
    <media id="video1" content-type="glencoe play-in-place height-250 width-310" xlink:href="http://cdn.elifesciences.org/video/eLifeLensIntro2.mp4" mimetype="video" mime-subtype="avi">
      <object-id pub-id-type="doi">10.7554/eLife.00116.004</object-id>
      <label>Video 1.</label>
      <caption>
        <title>Introducing eLife Lens.</title>
        <p>Watch Ian Mulvany from eLife demonstrating Lens.</p>
      </caption>
    </media>
    <fig id="fig1">
      <label>Figure 1.</label>
      <caption>
        <title>The Lens Article Format</title>
        <p>An example of how to begin creating content for Lens. The backbone organizes all of the data in a readily accessible manner. The text node defines the paragraph that may reference figures or publications. The figure node defines the image that will be displayed while the annotation links the figure to the associated text node that references the figure. See the <ext-link xlink:href="http://lens.substance.io/#lens/lens_article">Lens Article Format</ext-link> for a more detailed explanation of all the possible node types.</p>
      </caption>
      <graphic xlink:href="data/about/figure_1.png"/>
    </fig>

    <fig id="fig2">
      <label>Figure 2.</label>
      <caption>
        <title>The workspace of Lens.</title>
        <p><bold>A.</bold> The document map. Each gray bar identifies an individual paragraph within the article. <bold>B.</bold> The articleâ€™s text content. <bold>C.</bold> All of the associated resources. This includes the Table of Contents, figures, references and additional information, including meta-data, about the article.</p>
      </caption>
      <graphic xlink:href="data/about/figure_2.jpg"/>
    </fig>

    <fig id="fig3">
      <label>Figure 3.</label>
      <caption>
        <title>Focus on a Figure</title>
        <p>Selecting a figure label within the text will bring that figure into view within the resources panel. The document map (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) will identify all of the other references to that figure by highlighting the paragraphs green.</p> 
      </caption>
      <graphic xlink:href="data/about/figure_3.png"/>
    </fig>

    <fig id="fig4">
      <label>Figure 4.</label>
      <caption>
        <title>Focus on a Citation</title>
        <p>Selecting a citation label within the text will bring that citation into view within the resources panel. The document map (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) will identify all of the other references to that citation by highlighting the paragraphs blue.</p>
      </caption>
      <graphic xlink:href="data/about/figure_4.png"/>
    </fig>

  </back>
</article>
